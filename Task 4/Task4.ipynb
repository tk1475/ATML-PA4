{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"\n",
        "ATML PA4 — Task 4 from scratch (no reuse):\n",
        "A clean, single-file PyTorch implementation of a small federated learning framework\n",
        "with four heterogeneity-mitigation strategies:\n",
        "- FedProx (local proximal regularization)\n",
        "- SCAFFOLD (control variates)\n",
        "- FedGH (server-side gradient harmonization)\n",
        "- FedSAM (sharpness-aware minimization on clients)\n",
        "\n",
        "\n",
        "Also includes:\n",
        "- CIFAR-10 loading and Dirichlet non-IID partitioning\n",
        "- Simple CNN model\n",
        "- FedAvg baseline\n",
        "- Weighted aggregation, client drift metric, logging hooks\n",
        "\n",
        "\n",
        "HOW TO USE (example):\n",
        "python ATML-PA4-Task4.py --strategy fedavg --alpha 0.1 --num-clients 10 --rounds 50 --K 5\n",
        "python ATML-PA4-Task4.py --strategy fedprox --mu 0.01 --alpha 0.1 --num-clients 10 --rounds 50 --K 5\n",
        "python ATML-PA4-Task4.py --strategy scaffold --alpha 0.1 --num-clients 10 --rounds 50 --K 5\n",
        "python ATML-PA4-Task4.py --strategy fedgh --alpha 0.1 --num-clients 10 --rounds 50 --K 5\n",
        "python ATML-PA4-Task4.py --strategy fedsam --rho 0.05 --alpha 0.1 --num-clients 10 --rounds 50 --K 5\n",
        "\n",
        "\n",
        "Notes:\n",
        "* Keep the model small to fit within assignment constraints.\n",
        "* SCAFFOLD doubles comms (sends control variates). FedSAM ~2x local compute.\n",
        "* FedGH adds O(M^2) server-time pairwise projections per round.\n",
        "\n",
        "\n",
        "This file is deliberately verbose and self-contained for clarity and grading.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Rd1DZxVkv98w"
      },
      "id": "Rd1DZxVkv98w"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6634b3d6",
      "metadata": {
        "id": "6634b3d6"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import argparse\n",
        "import copy\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple, Iterable, Optional\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "# torchvision is permissible for CIFAR-10\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Reproducibility helpers\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "  random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  os.environ[\"PYTHONHASHSEED\"] = str(seed)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Simple CNN for CIFAR-10\n",
        "# ---------------------------\n",
        "class SmallCNN(nn.Module):\n",
        "    def __init__(self, num_classes: int = 10):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),  # 16x16\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),  # 8x8\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64 * 8 * 8, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(256, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Dirichlet non-IID partition\n",
        "# ---------------------------\n",
        "\n",
        "def dirichlet_partition_indices(\n",
        "    targets: torch.Tensor, num_clients: int, alpha: float, seed: int = 42\n",
        ") -> List[List[int]]:\n",
        "    \"\"\"Split dataset indices into num_clients using class-wise Dirichlet(α) proportions.\n",
        "    Smaller α => higher label skew.\n",
        "    \"\"\"\n",
        "    g = torch.Generator().manual_seed(seed)\n",
        "    num_classes = int(targets.max().item() + 1)\n",
        "    class_indices = [torch.where(targets == c)[0].tolist() for c in range(num_classes)]\n",
        "    for ci in class_indices:\n",
        "        random.shuffle(ci)\n",
        "\n",
        "    client_indices = [[] for _ in range(num_clients)]\n",
        "\n",
        "    for c in range(num_classes):\n",
        "        # sample proportions for this class\n",
        "        proportions = torch.distributions.Dirichlet(torch.full((num_clients,), alpha)).sample()\n",
        "        proportions = (proportions / proportions.sum()).tolist()\n",
        "        cls_ids = class_indices[c]\n",
        "        # split cls_ids according to proportions\n",
        "        prev = 0\n",
        "        for k in range(num_clients):\n",
        "            take = int(round(proportions[k] * len(cls_ids)))\n",
        "            client_indices[k].extend(cls_ids[prev : prev + take])\n",
        "            prev += take\n",
        "        # in case of rounding leftovers, dump remainder into last client\n",
        "        if prev < len(cls_ids):\n",
        "            client_indices[-1].extend(cls_ids[prev:])\n",
        "\n",
        "    # shuffle each client list\n",
        "    for k in range(num_clients):\n",
        "        random.shuffle(client_indices[k])\n",
        "    return client_indices\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Utilities for (de)flattening params and deltas\n",
        "# ---------------------------\n",
        "\n",
        "def get_model_params_vector(model: nn.Module) -> torch.Tensor:\n",
        "    return torch.cat([p.detach().view(-1) for p in model.parameters()])\n",
        "\n",
        "\n",
        "def get_model_grads_vector(model: nn.Module) -> torch.Tensor:\n",
        "    return torch.cat([p.grad.detach().view(-1) if p.grad is not None else torch.zeros_like(p).view(-1) for p in model.parameters()])\n",
        "\n",
        "\n",
        "def assign_params_from_vector(model: nn.Module, vec: torch.Tensor):\n",
        "    offset = 0\n",
        "    with torch.no_grad():\n",
        "        for p in model.parameters():\n",
        "            numel = p.numel()\n",
        "            p.copy_(vec[offset : offset + numel].view_as(p))\n",
        "            offset += numel\n",
        "\n",
        "\n",
        "def add_inplace(tensors: Iterable[torch.Tensor], alphas: Iterable[float], out: torch.Tensor):\n",
        "    \"\"\"out = sum(alpha_i * tensor_i). Assumes flat vectors of equal shape.\"\"\"\n",
        "    out.zero_()\n",
        "    for t, a in zip(tensors, alphas):\n",
        "        out.add_(t, alpha=a)"
      ],
      "metadata": {
        "id": "Fax6TzrSUcou"
      },
      "id": "Fax6TzrSUcou",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Client logic (baseline + hooks)\n",
        "# ---------------------------\n",
        "@dataclass\n",
        "class ClientConfig:\n",
        "    lr: float = 0.01\n",
        "    momentum: float = 0.9\n",
        "    batch_size: int = 64\n",
        "    local_epochs: int = 5  # K\n",
        "    mu: float = 0.0  # for FedProx\n",
        "    rho: float = 0.0  # for FedSAM\n",
        "\n",
        "\n",
        "class Client:\n",
        "    def __init__(\n",
        "        self,\n",
        "        cid: int,\n",
        "        dataset: torch.utils.data.Dataset,\n",
        "        indices: List[int],\n",
        "        device: torch.device,\n",
        "        cfg: ClientConfig,\n",
        "        strategy: str,\n",
        "        model_template: nn.Module,\n",
        "        scaffold_ci_template: Optional[List[torch.Tensor]] = None,\n",
        "    ):\n",
        "        self.cid = cid\n",
        "        self.device = device\n",
        "        self.cfg = cfg\n",
        "        self.strategy = strategy.lower()\n",
        "        self.loader = DataLoader(Subset(dataset, indices), batch_size=cfg.batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "        self.model = copy.deepcopy(model_template).to(device)\n",
        "        # SCAFFOLD control variate for this client (list of tensors matching params)\n",
        "        if self.strategy == \"scaffold\":\n",
        "            assert scaffold_ci_template is not None\n",
        "            self.ci = [torch.zeros_like(t, device=device) for t in scaffold_ci_template]\n",
        "        else:\n",
        "            self.ci = None\n",
        "\n",
        "    def set_model_from_global(self, global_model: nn.Module):\n",
        "        self.model.load_state_dict(copy.deepcopy(global_model.state_dict()))\n",
        "\n",
        "    def _scaffold_apply_correction(self, model: nn.Module, c_global: List[torch.Tensor]):\n",
        "        # add (ci - c) to each parameter's gradient\n",
        "        with torch.no_grad():\n",
        "            for (p, gi, cg) in zip(model.parameters(), self.ci, c_global):\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                p.grad.add_(gi - cg)\n",
        "\n",
        "    def _fedprox_add_proximal(self, model: nn.Module, global_params: List[torch.Tensor]):\n",
        "        # add µ/2 * ||theta - theta_g||^2 to loss => grads add µ*(theta - theta_g)\n",
        "        mu = self.cfg.mu\n",
        "        if mu <= 0:\n",
        "            return 0.0\n",
        "        prox = 0.0\n",
        "        for p, g in zip(model.parameters(), global_params):\n",
        "            prox = prox + 0.5 * mu * torch.sum((p - g) ** 2)\n",
        "        return prox\n",
        "\n",
        "    def _fedsam_ascent(self, model: nn.Module, rho: float):\n",
        "        # Perturb weights: w_adv = w + rho * g/||g|| (g is grad w.r.t current w)\n",
        "        grad_vec = get_model_grads_vector(model)\n",
        "        eps = 1e-12\n",
        "        scale = rho / (grad_vec.norm(p=2) + eps)\n",
        "        offset = 0\n",
        "        with torch.no_grad():\n",
        "            for p in model.parameters():\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                numel = p.numel()\n",
        "                p.add_(grad_vec[offset : offset + numel].view_as(p), alpha=scale)\n",
        "                offset += numel\n",
        "\n",
        "    def _fedsam_descent_restore(self, model: nn.Module, rho: float):\n",
        "        # Undo the perturbation by subtracting same delta applied in ascent.\n",
        "        # NOTE: We recompute using the *current* grad vector, which is at w_adv; to precisely undo, we stored nothing.\n",
        "        # A more exact impl would store the ascent delta. We'll compute it again from grads-at-w (before ascent),\n",
        "        # but we no longer have those grads. So we do the simple approach: store ascent deltas.\n",
        "        pass  # We'll store deltas explicitly below.\n",
        "\n",
        "    def local_train(\n",
        "        self,\n",
        "        global_model: nn.Module,\n",
        "        c_global: Optional[List[torch.Tensor]] = None,\n",
        "    ) -> Dict[str, torch.Tensor | List[torch.Tensor]]:\n",
        "        device = self.device\n",
        "        self.set_model_from_global(global_model)\n",
        "        model = self.model\n",
        "        model.train()\n",
        "        opt = optim.SGD(model.parameters(), lr=self.cfg.lr, momentum=self.cfg.momentum)\n",
        "\n",
        "        # cache global params for FedProx gradient contribution\n",
        "        global_params = [p.detach().clone() for p in global_model.parameters()]\n",
        "\n",
        "        rho = self.cfg.rho if self.strategy == \"fedsam\" else 0.0\n",
        "\n",
        "        for ep in range(self.cfg.local_epochs):\n",
        "            for xb, yb in self.loader:\n",
        "                xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
        "\n",
        "                # ----- standard forward/backward -----\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                logits = model(xb)\n",
        "                loss = F.cross_entropy(logits, yb)\n",
        "\n",
        "                # FedProx proximal term\n",
        "                if self.strategy == \"fedprox\" and self.cfg.mu > 0:\n",
        "                    loss = loss + self._fedprox_add_proximal(model, global_params)\n",
        "\n",
        "                loss.backward()\n",
        "\n",
        "                # SCAFFOLD gradient correction\n",
        "                if self.strategy == \"scaffold\":\n",
        "                    assert c_global is not None and self.ci is not None\n",
        "                    self._scaffold_apply_correction(model, c_global)\n",
        "\n",
        "                # FedSAM two-step\n",
        "                if self.strategy == \"fedsam\" and rho > 0:\n",
        "                    # store ascent deltas\n",
        "                    ascent_deltas = []\n",
        "                    with torch.no_grad():\n",
        "                        grad_vec = get_model_grads_vector(model)\n",
        "                        eps = 1e-12\n",
        "                        scale = rho / (grad_vec.norm(p=2) + eps)\n",
        "                        offset = 0\n",
        "                        for p in model.parameters():\n",
        "                            if p.grad is None:\n",
        "                                ascent_deltas.append(None)\n",
        "                                continue\n",
        "                            numel = p.numel()\n",
        "                            delta = grad_vec[offset : offset + numel].view_as(p) * scale\n",
        "                            p.add_(delta)\n",
        "                            ascent_deltas.append(delta)\n",
        "                            offset += numel\n",
        "\n",
        "                    # compute grad at perturbed weights\n",
        "                    opt.zero_grad(set_to_none=True)\n",
        "                    logits_adv = model(xb)\n",
        "                    loss_adv = F.cross_entropy(logits_adv, yb)\n",
        "                    loss_adv.backward()\n",
        "\n",
        "                    # restore original weights\n",
        "                    with torch.no_grad():\n",
        "                        for p, delta in zip(model.parameters(), ascent_deltas):\n",
        "                            if delta is not None:\n",
        "                                p.sub_(delta)\n",
        "\n",
        "                    # now apply optimizer step using grads at w_adv (stored on params)\n",
        "                    opt.step()\n",
        "                else:\n",
        "                    # vanilla or FedProx/SCAFFOLD (after correction)\n",
        "                    opt.step()\n",
        "\n",
        "        # return results\n",
        "        with torch.no_grad():\n",
        "            theta_i = [p.detach().clone() for p in model.parameters()]\n",
        "            theta_g = [p.detach().clone() for p in global_model.parameters()]\n",
        "            # update delta for aggregation\n",
        "            deltas = [ti - tg for ti, tg in zip(theta_i, theta_g)]\n",
        "\n",
        "        out: Dict[str, torch.Tensor | List[torch.Tensor]] = {\n",
        "            \"params\": theta_i,\n",
        "            \"delta\": deltas,\n",
        "            \"num_samples\": torch.tensor(len(self.loader.dataset), dtype=torch.long),\n",
        "        }\n",
        "\n",
        "        # SCAFFOLD: update ci based on global and local change\n",
        "        if self.strategy == \"scaffold\":\n",
        "            assert self.ci is not None and c_global is not None\n",
        "            # ci <- c + (1 / (K * lr)) * (theta_g - theta_i)\n",
        "            K = self.cfg.local_epochs\n",
        "            lr = self.cfg.lr\n",
        "            with torch.no_grad():\n",
        "                for idx, (gi, cg, tg, ti) in enumerate(zip(self.ci, c_global, theta_g, theta_i)):\n",
        "                    gi.copy_(cg + (tg - ti) / (K * lr))\n",
        "            out[\"ci\"] = [t.detach().clone() for t in self.ci]\n",
        "\n",
        "        return out\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3MzlH8atUn3-"
      },
      "id": "3MzlH8atUn3-",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Server & strategies\n",
        "# ---------------------------\n",
        "class Server:\n",
        "    def __init__(self, model: nn.Module, device: torch.device):\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "\n",
        "    def aggregate_weighted(self, client_params: List[List[torch.Tensor]], weights: List[float]):\n",
        "        with torch.no_grad():\n",
        "            for p_idx, p in enumerate(self.model.parameters()):\n",
        "                acc = None\n",
        "                for w, params in zip(weights, client_params):\n",
        "                    term = params[p_idx].to(self.device) * w\n",
        "                    acc = term if acc is None else acc + term\n",
        "                p.copy_(acc)\n",
        "\n",
        "    def aggregate_from_deltas(self, deltas: List[List[torch.Tensor]], weights: List[float]):\n",
        "        with torch.no_grad():\n",
        "            for p_idx, p in enumerate(self.model.parameters()):\n",
        "                acc = torch.zeros_like(p)\n",
        "                for w, dlist in zip(weights, deltas):\n",
        "                    acc.add_(dlist[p_idx].to(self.device), alpha=w)\n",
        "                p.add_(acc)\n",
        "\n",
        "    # FedGH: harmonize deltas before averaging\n",
        "    def harmonize_pairwise(self, flat_updates: List[torch.Tensor]) -> List[torch.Tensor]:\n",
        "        M = len(flat_updates)\n",
        "        outs = [u.clone() for u in flat_updates]\n",
        "        for i in range(M):\n",
        "            for j in range(i + 1, M):\n",
        "                gi, gj = outs[i], outs[j]\n",
        "                dot = torch.dot(gi, gj)\n",
        "                if dot < 0:\n",
        "                    # project symmetric\n",
        "                    gi_norm2 = torch.dot(gi, gi) + 1e-12\n",
        "                    gj_norm2 = torch.dot(gj, gj) + 1e-12\n",
        "                    proj_i = dot / gj_norm2\n",
        "                    proj_j = dot / gi_norm2\n",
        "                    outs[i] = gi - proj_i * gj\n",
        "                    outs[j] = gj - proj_j * gi\n",
        "        return outs\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Evaluation & metrics\n",
        "# ---------------------------\n",
        "@torch.no_grad()\n",
        "def evaluate(model: nn.Module, loader: DataLoader, device: torch.device) -> Tuple[float, float]:\n",
        "    model.eval()\n",
        "    total, correct, loss_sum = 0, 0, 0.0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        logits = model(xb)\n",
        "        loss = F.cross_entropy(logits, yb, reduction='sum')\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct += (preds == yb).sum().item()\n",
        "        total += yb.size(0)\n",
        "        loss_sum += loss.item()\n",
        "    return correct / total, loss_sum / total\n",
        "\n",
        "\n",
        "def compute_drift(global_model: nn.Module, client_param_lists: List[List[torch.Tensor]], device: torch.device) -> float:\n",
        "    with torch.no_grad():\n",
        "        gparams = [p.detach().to(device) for p in global_model.parameters()]\n",
        "        dists = []\n",
        "        for plist in client_param_lists:\n",
        "            s = 0.0\n",
        "            for gp, cp in zip(gparams, plist):\n",
        "                s += torch.norm(cp.to(device) - gp, p=2).item() ** 2\n",
        "            dists.append(math.sqrt(s))\n",
        "        return float(sum(dists) / len(dists))\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Data loading\n",
        "# ---------------------------\n",
        "\n",
        "def get_cifar10(root: str = \"./data\"):\n",
        "    tfm_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "    ])\n",
        "    tfm_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "    ])\n",
        "    train = datasets.CIFAR10(root, train=True, download=True, transform=tfm_train)\n",
        "    test = datasets.CIFAR10(root, train=False, download=True, transform=tfm_test)\n",
        "    return train, test\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fMlOPsnMUrKr"
      },
      "id": "fMlOPsnMUrKr",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #---------------------------\n",
        "# Training orchestration\n",
        "# ---------------------------\n",
        "\n",
        "def run(\n",
        "    strategy: str,\n",
        "    num_clients: int,\n",
        "    alpha: float,\n",
        "    rounds: int,\n",
        "    K: int,\n",
        "    batch_size: int,\n",
        "    lr: float,\n",
        "    momentum: float,\n",
        "    mu: float,\n",
        "    rho: float,\n",
        "    sample_frac: float,\n",
        "    seed: int = 42,\n",
        "    device_str: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "):\n",
        "    set_seed(seed)\n",
        "    device = torch.device(device_str)\n",
        "\n",
        "    # data\n",
        "    train_set, test_set = get_cifar10()\n",
        "    test_loader = DataLoader(test_set, batch_size=256, shuffle=False, num_workers=2)\n",
        "\n",
        "    # partition\n",
        "    targets = torch.tensor(train_set.targets)\n",
        "    splits = dirichlet_partition_indices(targets, num_clients=num_clients, alpha=alpha, seed=seed)\n",
        "\n",
        "    # model template\n",
        "    global_model = SmallCNN().to(device)\n",
        "    server = Server(global_model, device)\n",
        "\n",
        "    # client configs\n",
        "    cfg = ClientConfig(lr=lr, momentum=momentum, batch_size=batch_size, local_epochs=K, mu=mu, rho=rho)\n",
        "\n",
        "    # SCAFFOLD templates\n",
        "    scaffold_template = None\n",
        "    if strategy.lower() == \"scaffold\":\n",
        "        scaffold_template = [p.detach().clone() for p in global_model.parameters()]\n",
        "\n",
        "    # build clients\n",
        "    clients: List[Client] = []\n",
        "    for cid in range(num_clients):\n",
        "        clients.append(\n",
        "            Client(\n",
        "                cid=cid,\n",
        "                dataset=train_set,\n",
        "                indices=splits[cid],\n",
        "                device=device,\n",
        "                cfg=cfg,\n",
        "                strategy=strategy,\n",
        "                model_template=global_model,\n",
        "                scaffold_ci_template=scaffold_template,\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # SCAFFOLD global control variate c\n",
        "    c_global: Optional[List[torch.Tensor]] = None\n",
        "    if strategy.lower() == \"scaffold\":\n",
        "        c_global = [torch.zeros_like(p, device=device) for p in global_model.parameters()]\n",
        "\n",
        "    # training loop\n",
        "    frac = sample_frac\n",
        "    for rnd in range(1, rounds + 1):\n",
        "        # sample participating clients\n",
        "        m = max(1, int(round(frac * num_clients)))\n",
        "        selected = random.sample(range(num_clients), m)\n",
        "\n",
        "        # broadcast implicit via copying in local_train\n",
        "        results = []\n",
        "        for idx in selected:\n",
        "            res = clients[idx].local_train(global_model, c_global)\n",
        "            results.append((idx, res))\n",
        "\n",
        "        # weights by client data size\n",
        "        sizes = [int(res[\"num_samples\"]) for _, res in results]\n",
        "        total = sum(sizes)\n",
        "        weights = [s / total for s in sizes]\n",
        "\n",
        "        # metrics: drift before aggregation (based on current local params)\n",
        "        drift_val = compute_drift(global_model, [res[\"params\"] for _, res in results], device)\n",
        "\n",
        "        # aggregation\n",
        "        if strategy.lower() == \"fedgh\":\n",
        "            # harmonize flat deltas then add to global\n",
        "            flat = []\n",
        "            for _, res in results:\n",
        "                # concat layers (weighted delta will be applied after harmonization via weights)\n",
        "                deltas = res[\"delta\"]\n",
        "                flat.append(torch.cat([d.detach().view(-1).to(device) for d in deltas]))\n",
        "            flat_h = server.harmonize_pairwise(flat)\n",
        "            # reconstruct per-layer from flat\n",
        "            # We'll distribute harmonized flat deltas proportionally by weights\n",
        "            # First, split shapes\n",
        "            shapes = [p.shape for p in global_model.parameters()]\n",
        "            sizes_layer = [int(torch.tensor(s).prod()) for s in shapes]\n",
        "            per_client_deltas: List[List[torch.Tensor]] = []\n",
        "            for fh in flat_h:\n",
        "                offset = 0\n",
        "                dl = []\n",
        "                for sz, shp in zip(sizes_layer, shapes):\n",
        "                    dl.append(fh[offset:offset+sz].view(shp))\n",
        "                    offset += sz\n",
        "                per_client_deltas.append(dl)\n",
        "            server.aggregate_from_deltas(per_client_deltas, weights)\n",
        "        else:\n",
        "            # standard weighted average on parameters (FedAvg-style)\n",
        "            server.aggregate_weighted([res[\"params\"] for _, res in results], weights)\n",
        "\n",
        "        # SCAFFOLD: update c_global to average of ci\n",
        "        if strategy.lower() == \"scaffold\":\n",
        "            with torch.no_grad():\n",
        "                agg_ci = None\n",
        "                for _, res in results:\n",
        "                    ci_list = res[\"ci\"]  # type: ignore\n",
        "                    agg_ci = [t.clone() for t in ci_list] if agg_ci is None else [a + b for a, b in zip(agg_ci, ci_list)]\n",
        "                for i in range(len(agg_ci)):\n",
        "                    agg_ci[i] = agg_ci[i] / len(results)\n",
        "                for i, p in enumerate(c_global):\n",
        "                    p.copy_(agg_ci[i].to(device))\n",
        "\n",
        "        # eval\n",
        "        acc, loss = evaluate(global_model, test_loader, device)\n",
        "        print(f\"Round {rnd:03d} | clients {m:02d}/{num_clients} | drift {drift_val:.3f} | acc {acc*100:.2f}% | loss {loss:.4f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cgY5T2WXUyAt"
      },
      "id": "cgY5T2WXUyAt",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the run function directly with desired parameters\n",
        "run(\n",
        "    strategy=\"fedavg\",  # default strategy\n",
        "    num_clients=10,    # default number of clients\n",
        "    alpha=0.1,         # default Dirichlet concentration\n",
        "    rounds=50,         # default number of rounds\n",
        "    K=5,               # default local epochs\n",
        "    batch_size=64,     # default batch size\n",
        "    lr=0.01,           # default learning rate\n",
        "    momentum=0.9,      # default momentum\n",
        "    mu=0.01,           # default FedProx mu\n",
        "    rho=0.05,          # default FedSAM rho\n",
        "    sample_frac=1.0,   # default sample fraction\n",
        "    seed=42,           # default seed\n",
        "    device_str=\"cuda\" if torch.cuda.is_available() else \"cpu\", # default device\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uub2dpw2VJ_g",
        "outputId": "40fe81cf-718e-449a-c4dd-6594433fbd02"
      },
      "id": "Uub2dpw2VJ_g",
      "execution_count": 6,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 44.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Round 001 | clients 10/10 | drift 3.102 | acc 33.26% | loss 2.1159\n",
            "Round 002 | clients 10/10 | drift 3.010 | acc 41.19% | loss 1.7394\n",
            "Round 003 | clients 10/10 | drift 2.849 | acc 49.18% | loss 1.4678\n",
            "Round 004 | clients 10/10 | drift 2.727 | acc 53.92% | loss 1.3078\n",
            "Round 005 | clients 10/10 | drift 2.689 | acc 58.48% | loss 1.2013\n",
            "Round 006 | clients 10/10 | drift 2.696 | acc 61.09% | loss 1.1091\n",
            "Round 007 | clients 10/10 | drift 2.667 | acc 63.22% | loss 1.0548\n",
            "Round 008 | clients 10/10 | drift 2.679 | acc 64.37% | loss 1.0264\n",
            "Round 009 | clients 10/10 | drift 2.682 | acc 65.99% | loss 0.9841\n",
            "Round 010 | clients 10/10 | drift 2.711 | acc 67.67% | loss 0.9363\n",
            "Round 011 | clients 10/10 | drift 2.740 | acc 68.26% | loss 0.9074\n",
            "Round 012 | clients 10/10 | drift 2.768 | acc 68.93% | loss 0.8963\n",
            "Round 013 | clients 10/10 | drift 2.841 | acc 69.33% | loss 0.8960\n",
            "Round 014 | clients 10/10 | drift 2.810 | acc 70.57% | loss 0.8575\n",
            "Round 015 | clients 10/10 | drift 2.844 | acc 70.61% | loss 0.8609\n",
            "Round 016 | clients 10/10 | drift 2.846 | acc 71.46% | loss 0.8281\n",
            "Round 017 | clients 10/10 | drift 2.929 | acc 71.05% | loss 0.8494\n",
            "Round 018 | clients 10/10 | drift 2.914 | acc 71.59% | loss 0.8263\n",
            "Round 019 | clients 10/10 | drift 2.979 | acc 71.70% | loss 0.8315\n",
            "Round 020 | clients 10/10 | drift 3.001 | acc 71.92% | loss 0.8181\n",
            "Round 021 | clients 10/10 | drift 3.001 | acc 72.25% | loss 0.8111\n",
            "Round 022 | clients 10/10 | drift 3.048 | acc 72.55% | loss 0.8060\n",
            "Round 023 | clients 10/10 | drift 3.051 | acc 73.21% | loss 0.7985\n",
            "Round 024 | clients 10/10 | drift 3.084 | acc 72.46% | loss 0.8193\n",
            "Round 025 | clients 10/10 | drift 3.105 | acc 72.81% | loss 0.8045\n",
            "Round 026 | clients 10/10 | drift 3.168 | acc 73.07% | loss 0.8046\n",
            "Round 027 | clients 10/10 | drift 3.092 | acc 72.93% | loss 0.8098\n",
            "Round 028 | clients 10/10 | drift 3.138 | acc 73.61% | loss 0.7978\n",
            "Round 029 | clients 10/10 | drift 3.196 | acc 73.46% | loss 0.8108\n",
            "Round 030 | clients 10/10 | drift 3.211 | acc 73.68% | loss 0.7927\n",
            "Round 031 | clients 10/10 | drift 3.182 | acc 73.53% | loss 0.8113\n",
            "Round 032 | clients 10/10 | drift 3.215 | acc 73.88% | loss 0.8005\n",
            "Round 033 | clients 10/10 | drift 3.265 | acc 73.44% | loss 0.8212\n",
            "Round 034 | clients 10/10 | drift 3.247 | acc 74.01% | loss 0.8142\n",
            "Round 035 | clients 10/10 | drift 3.254 | acc 73.62% | loss 0.8095\n",
            "Round 036 | clients 10/10 | drift 3.331 | acc 74.37% | loss 0.8043\n",
            "Round 037 | clients 10/10 | drift 3.290 | acc 74.12% | loss 0.8074\n",
            "Round 038 | clients 10/10 | drift 3.325 | acc 73.78% | loss 0.8202\n",
            "Round 039 | clients 10/10 | drift 3.296 | acc 73.72% | loss 0.8280\n",
            "Round 040 | clients 10/10 | drift 3.353 | acc 74.01% | loss 0.8195\n",
            "Round 041 | clients 10/10 | drift 3.356 | acc 74.43% | loss 0.8212\n",
            "Round 042 | clients 10/10 | drift 3.393 | acc 74.44% | loss 0.8187\n",
            "Round 043 | clients 10/10 | drift 3.355 | acc 74.28% | loss 0.8253\n",
            "Round 044 | clients 10/10 | drift 3.342 | acc 74.52% | loss 0.8371\n",
            "Round 045 | clients 10/10 | drift 3.492 | acc 74.58% | loss 0.8254\n",
            "Round 046 | clients 10/10 | drift 3.393 | acc 73.77% | loss 0.8539\n",
            "Round 047 | clients 10/10 | drift 3.390 | acc 73.51% | loss 0.8593\n",
            "Round 048 | clients 10/10 | drift 3.432 | acc 73.86% | loss 0.8717\n",
            "Round 049 | clients 10/10 | drift 3.430 | acc 74.29% | loss 0.8543\n",
            "Round 050 | clients 10/10 | drift 3.397 | acc 74.42% | loss 0.8586\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "strategies = [\n",
        "{\"strategy\": \"fedavg\", \"label\": \"FedAvg\"},\n",
        "{\"strategy\": \"fedprox\", \"label\": \"FedProx\", \"mu\": 0.01},\n",
        "{\"strategy\": \"scaffold\", \"label\": \"SCAFFOLD\"},\n",
        "{\"strategy\": \"fedgh\", \"label\": \"FedGH\"},\n",
        "{\"strategy\": \"fedsam\", \"label\": \"FedSAM\", \"rho\": 0.05},\n",
        "]\n",
        "\n",
        "\n",
        "common_cfg = dict(\n",
        "num_clients=10,\n",
        "alpha=0.1,\n",
        "rounds=50,\n",
        "K=5,\n",
        "batch_size=64,\n",
        "lr=0.01,\n",
        "momentum=0.9,\n",
        "sample_frac=1.0,\n",
        "seed=42,\n",
        ")\n",
        "\n",
        "\n",
        "for cfg in strategies:\n",
        "label = cfg.pop(\"label\")\n",
        "print(f\"\\n{'='*80}\\nRunning {label}\\n{'='*80}\")\n",
        "run(**common_cfg, **cfg)"
      ],
      "metadata": {
        "id": "1ZLVywsw2VtZ"
      },
      "id": "1ZLVywsw2VtZ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}