{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"\n",
        "ATML PA4 — Task 4 from scratch (no reuse):\n",
        "A clean, single-file PyTorch implementation of a small federated learning framework\n",
        "with four heterogeneity-mitigation strategies:\n",
        "- FedProx (local proximal regularization)\n",
        "- SCAFFOLD (control variates)\n",
        "- FedGH (server-side gradient harmonization)\n",
        "- FedSAM (sharpness-aware minimization on clients)\n",
        "\n",
        "\n",
        "Also includes:\n",
        "- CIFAR-10 loading and Dirichlet non-IID partitioning\n",
        "- Simple CNN model\n",
        "- FedAvg baseline\n",
        "- Weighted aggregation, client drift metric, logging hooks\n",
        "\n",
        "\n",
        "HOW TO USE (example):\n",
        "python ATML-PA4-Task4.py --strategy fedavg --alpha 0.1 --num-clients 10 --rounds 50 --K 5\n",
        "python ATML-PA4-Task4.py --strategy fedprox --mu 0.01 --alpha 0.1 --num-clients 10 --rounds 50 --K 5\n",
        "python ATML-PA4-Task4.py --strategy scaffold --alpha 0.1 --num-clients 10 --rounds 50 --K 5\n",
        "python ATML-PA4-Task4.py --strategy fedgh --alpha 0.1 --num-clients 10 --rounds 50 --K 5\n",
        "python ATML-PA4-Task4.py --strategy fedsam --rho 0.05 --alpha 0.1 --num-clients 10 --rounds 50 --K 5\n",
        "\n",
        "\n",
        "Notes:\n",
        "* Keep the model small to fit within assignment constraints.\n",
        "* SCAFFOLD doubles comms (sends control variates). FedSAM ~2x local compute.\n",
        "* FedGH adds O(M^2) server-time pairwise projections per round.\n",
        "\n",
        "\n",
        "This file is deliberately verbose and self-contained for clarity and grading.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Rd1DZxVkv98w"
      },
      "id": "Rd1DZxVkv98w"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6634b3d6",
      "metadata": {
        "id": "6634b3d6"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import argparse\n",
        "import copy\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple, Iterable, Optional\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "# torchvision is permissible for CIFAR-10\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Reproducibility helpers\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "  random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  os.environ[\"PYTHONHASHSEED\"] = str(seed)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Simple CNN for CIFAR-10\n",
        "# ---------------------------\n",
        "class SmallCNN(nn.Module):\n",
        "    def __init__(self, num_classes: int = 10):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),  # 16x16\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),  # 8x8\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64 * 8 * 8, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(256, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Dirichlet non-IID partition\n",
        "# ---------------------------\n",
        "\n",
        "def dirichlet_partition_indices(\n",
        "    targets: torch.Tensor, num_clients: int, alpha: float, seed: int = 42\n",
        ") -> List[List[int]]:\n",
        "    \"\"\"Split dataset indices into num_clients using class-wise Dirichlet(α) proportions.\n",
        "    Smaller α => higher label skew.\n",
        "    \"\"\"\n",
        "    g = torch.Generator().manual_seed(seed)\n",
        "    num_classes = int(targets.max().item() + 1)\n",
        "    class_indices = [torch.where(targets == c)[0].tolist() for c in range(num_classes)]\n",
        "    for ci in class_indices:\n",
        "        random.shuffle(ci)\n",
        "\n",
        "    client_indices = [[] for _ in range(num_clients)]\n",
        "\n",
        "    for c in range(num_classes):\n",
        "        # sample proportions for this class\n",
        "        proportions = torch.distributions.Dirichlet(torch.full((num_clients,), alpha)).sample()\n",
        "        proportions = (proportions / proportions.sum()).tolist()\n",
        "        cls_ids = class_indices[c]\n",
        "        # split cls_ids according to proportions\n",
        "        prev = 0\n",
        "        for k in range(num_clients):\n",
        "            take = int(round(proportions[k] * len(cls_ids)))\n",
        "            client_indices[k].extend(cls_ids[prev : prev + take])\n",
        "            prev += take\n",
        "        # in case of rounding leftovers, dump remainder into last client\n",
        "        if prev < len(cls_ids):\n",
        "            client_indices[-1].extend(cls_ids[prev:])\n",
        "\n",
        "    # shuffle each client list\n",
        "    for k in range(num_clients):\n",
        "        random.shuffle(client_indices[k])\n",
        "    return client_indices\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Utilities for (de)flattening params and deltas\n",
        "# ---------------------------\n",
        "\n",
        "def get_model_params_vector(model: nn.Module) -> torch.Tensor:\n",
        "    return torch.cat([p.detach().view(-1) for p in model.parameters()])\n",
        "\n",
        "\n",
        "def get_model_grads_vector(model: nn.Module) -> torch.Tensor:\n",
        "    return torch.cat([p.grad.detach().view(-1) if p.grad is not None else torch.zeros_like(p).view(-1) for p in model.parameters()])\n",
        "\n",
        "\n",
        "def assign_params_from_vector(model: nn.Module, vec: torch.Tensor):\n",
        "    offset = 0\n",
        "    with torch.no_grad():\n",
        "        for p in model.parameters():\n",
        "            numel = p.numel()\n",
        "            p.copy_(vec[offset : offset + numel].view_as(p))\n",
        "            offset += numel\n",
        "\n",
        "\n",
        "def add_inplace(tensors: Iterable[torch.Tensor], alphas: Iterable[float], out: torch.Tensor):\n",
        "    \"\"\"out = sum(alpha_i * tensor_i). Assumes flat vectors of equal shape.\"\"\"\n",
        "    out.zero_()\n",
        "    for t, a in zip(tensors, alphas):\n",
        "        out.add_(t, alpha=a)"
      ],
      "metadata": {
        "id": "Fax6TzrSUcou"
      },
      "id": "Fax6TzrSUcou",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Client logic (baseline + hooks)\n",
        "# ---------------------------\n",
        "@dataclass\n",
        "class ClientConfig:\n",
        "    lr: float = 0.01\n",
        "    momentum: float = 0.9\n",
        "    batch_size: int = 64\n",
        "    local_epochs: int = 5  # K\n",
        "    mu: float = 0.0  # for FedProx\n",
        "    rho: float = 0.0  # for FedSAM\n",
        "\n",
        "\n",
        "class Client:\n",
        "    def __init__(\n",
        "        self,\n",
        "        cid: int,\n",
        "        dataset: torch.utils.data.Dataset,\n",
        "        indices: List[int],\n",
        "        device: torch.device,\n",
        "        cfg: ClientConfig,\n",
        "        strategy: str,\n",
        "        model_template: nn.Module,\n",
        "        scaffold_ci_template: Optional[List[torch.Tensor]] = None,\n",
        "    ):\n",
        "        self.cid = cid\n",
        "        self.device = device\n",
        "        self.cfg = cfg\n",
        "        self.strategy = strategy.lower()\n",
        "        self.loader = DataLoader(Subset(dataset, indices), batch_size=cfg.batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "        self.model = copy.deepcopy(model_template).to(device)\n",
        "        # SCAFFOLD control variate for this client (list of tensors matching params)\n",
        "        if self.strategy == \"scaffold\":\n",
        "            assert scaffold_ci_template is not None\n",
        "            self.ci = [torch.zeros_like(t, device=device) for t in scaffold_ci_template]\n",
        "        else:\n",
        "            self.ci = None\n",
        "\n",
        "    def set_model_from_global(self, global_model: nn.Module):\n",
        "        self.model.load_state_dict(copy.deepcopy(global_model.state_dict()))\n",
        "\n",
        "    def _scaffold_apply_correction(self, model: nn.Module, c_global: List[torch.Tensor]):\n",
        "        # add (ci - c) to each parameter's gradient\n",
        "        with torch.no_grad():\n",
        "            for (p, gi, cg) in zip(model.parameters(), self.ci, c_global):\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                p.grad.add_(gi - cg)\n",
        "\n",
        "    def _fedprox_add_proximal(self, model: nn.Module, global_params: List[torch.Tensor]):\n",
        "        # add µ/2 * ||theta - theta_g||^2 to loss => grads add µ*(theta - theta_g)\n",
        "        mu = self.cfg.mu\n",
        "        if mu <= 0:\n",
        "            return 0.0\n",
        "        prox = 0.0\n",
        "        for p, g in zip(model.parameters(), global_params):\n",
        "            prox = prox + 0.5 * mu * torch.sum((p - g) ** 2)\n",
        "        return prox\n",
        "\n",
        "    def _fedsam_ascent(self, model: nn.Module, rho: float):\n",
        "        # Perturb weights: w_adv = w + rho * g/||g|| (g is grad w.r.t current w)\n",
        "        grad_vec = get_model_grads_vector(model)\n",
        "        eps = 1e-12\n",
        "        scale = rho / (grad_vec.norm(p=2) + eps)\n",
        "        offset = 0\n",
        "        with torch.no_grad():\n",
        "            for p in model.parameters():\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                numel = p.numel()\n",
        "                p.add_(grad_vec[offset : offset + numel].view_as(p), alpha=scale)\n",
        "                offset += numel\n",
        "\n",
        "    def _fedsam_descent_restore(self, model: nn.Module, rho: float):\n",
        "        # Undo the perturbation by subtracting same delta applied in ascent.\n",
        "        # NOTE: We recompute using the *current* grad vector, which is at w_adv; to precisely undo, we stored nothing.\n",
        "        # A more exact impl would store the ascent delta. We'll compute it again from grads-at-w (before ascent),\n",
        "        # but we no longer have those grads. So we do the simple approach: store ascent deltas.\n",
        "        pass  # We'll store deltas explicitly below.\n",
        "\n",
        "    def local_train(\n",
        "        self,\n",
        "        global_model: nn.Module,\n",
        "        c_global: Optional[List[torch.Tensor]] = None,\n",
        "    ) -> Dict[str, torch.Tensor | List[torch.Tensor]]:\n",
        "        device = self.device\n",
        "        self.set_model_from_global(global_model)\n",
        "        model = self.model\n",
        "        model.train()\n",
        "        opt = optim.SGD(model.parameters(), lr=self.cfg.lr, momentum=self.cfg.momentum)\n",
        "\n",
        "        # cache global params for FedProx gradient contribution\n",
        "        global_params = [p.detach().clone() for p in global_model.parameters()]\n",
        "\n",
        "        rho = self.cfg.rho if self.strategy == \"fedsam\" else 0.0\n",
        "\n",
        "        for ep in range(self.cfg.local_epochs):\n",
        "            for xb, yb in self.loader:\n",
        "                xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
        "\n",
        "                # ----- standard forward/backward -----\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                logits = model(xb)\n",
        "                loss = F.cross_entropy(logits, yb)\n",
        "\n",
        "                # FedProx proximal term\n",
        "                if self.strategy == \"fedprox\" and self.cfg.mu > 0:\n",
        "                    loss = loss + self._fedprox_add_proximal(model, global_params)\n",
        "\n",
        "                loss.backward()\n",
        "\n",
        "                # SCAFFOLD gradient correction\n",
        "                if self.strategy == \"scaffold\":\n",
        "                    assert c_global is not None and self.ci is not None\n",
        "                    self._scaffold_apply_correction(model, c_global)\n",
        "\n",
        "                # FedSAM two-step\n",
        "                if self.strategy == \"fedsam\" and rho > 0:\n",
        "                    # store ascent deltas\n",
        "                    ascent_deltas = []\n",
        "                    with torch.no_grad():\n",
        "                        grad_vec = get_model_grads_vector(model)\n",
        "                        eps = 1e-12\n",
        "                        scale = rho / (grad_vec.norm(p=2) + eps)\n",
        "                        offset = 0\n",
        "                        for p in model.parameters():\n",
        "                            if p.grad is None:\n",
        "                                ascent_deltas.append(None)\n",
        "                                continue\n",
        "                            numel = p.numel()\n",
        "                            delta = grad_vec[offset : offset + numel].view_as(p) * scale\n",
        "                            p.add_(delta)\n",
        "                            ascent_deltas.append(delta)\n",
        "                            offset += numel\n",
        "\n",
        "                    # compute grad at perturbed weights\n",
        "                    opt.zero_grad(set_to_none=True)\n",
        "                    logits_adv = model(xb)\n",
        "                    loss_adv = F.cross_entropy(logits_adv, yb)\n",
        "                    loss_adv.backward()\n",
        "\n",
        "                    # restore original weights\n",
        "                    with torch.no_grad():\n",
        "                        for p, delta in zip(model.parameters(), ascent_deltas):\n",
        "                            if delta is not None:\n",
        "                                p.sub_(delta)\n",
        "\n",
        "                    # now apply optimizer step using grads at w_adv (stored on params)\n",
        "                    opt.step()\n",
        "                else:\n",
        "                    # vanilla or FedProx/SCAFFOLD (after correction)\n",
        "                    opt.step()\n",
        "\n",
        "        # return results\n",
        "        with torch.no_grad():\n",
        "            theta_i = [p.detach().clone() for p in model.parameters()]\n",
        "            theta_g = [p.detach().clone() for p in global_model.parameters()]\n",
        "            # update delta for aggregation\n",
        "            deltas = [ti - tg for ti, tg in zip(theta_i, theta_g)]\n",
        "\n",
        "        out: Dict[str, torch.Tensor | List[torch.Tensor]] = {\n",
        "            \"params\": theta_i,\n",
        "            \"delta\": deltas,\n",
        "            \"num_samples\": torch.tensor(len(self.loader.dataset), dtype=torch.long),\n",
        "        }\n",
        "\n",
        "        # SCAFFOLD: update ci based on global and local change\n",
        "        if self.strategy == \"scaffold\":\n",
        "            assert self.ci is not None and c_global is not None\n",
        "            # ci <- c + (1 / (K * lr)) * (theta_g - theta_i)\n",
        "            K = self.cfg.local_epochs\n",
        "            lr = self.cfg.lr\n",
        "            with torch.no_grad():\n",
        "                for idx, (gi, cg, tg, ti) in enumerate(zip(self.ci, c_global, theta_g, theta_i)):\n",
        "                    gi.copy_(cg + (tg - ti) / (K * lr))\n",
        "            out[\"ci\"] = [t.detach().clone() for t in self.ci]\n",
        "\n",
        "        return out\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3MzlH8atUn3-"
      },
      "id": "3MzlH8atUn3-",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Server & strategies\n",
        "# ---------------------------\n",
        "class Server:\n",
        "    def __init__(self, model: nn.Module, device: torch.device):\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "\n",
        "    def aggregate_weighted(self, client_params: List[List[torch.Tensor]], weights: List[float]):\n",
        "        with torch.no_grad():\n",
        "            for p_idx, p in enumerate(self.model.parameters()):\n",
        "                acc = None\n",
        "                for w, params in zip(weights, client_params):\n",
        "                    term = params[p_idx].to(self.device) * w\n",
        "                    acc = term if acc is None else acc + term\n",
        "                p.copy_(acc)\n",
        "\n",
        "    def aggregate_from_deltas(self, deltas: List[List[torch.Tensor]], weights: List[float]):\n",
        "        with torch.no_grad():\n",
        "            for p_idx, p in enumerate(self.model.parameters()):\n",
        "                acc = torch.zeros_like(p)\n",
        "                for w, dlist in zip(weights, deltas):\n",
        "                    acc.add_(dlist[p_idx].to(self.device), alpha=w)\n",
        "                p.add_(acc)\n",
        "\n",
        "    # FedGH: harmonize deltas before averaging\n",
        "    def harmonize_pairwise(self, flat_updates: List[torch.Tensor]) -> List[torch.Tensor]:\n",
        "        M = len(flat_updates)\n",
        "        outs = [u.clone() for u in flat_updates]\n",
        "        for i in range(M):\n",
        "            for j in range(i + 1, M):\n",
        "                gi, gj = outs[i], outs[j]\n",
        "                dot = torch.dot(gi, gj)\n",
        "                if dot < 0:\n",
        "                    # project symmetric\n",
        "                    gi_norm2 = torch.dot(gi, gi) + 1e-12\n",
        "                    gj_norm2 = torch.dot(gj, gj) + 1e-12\n",
        "                    proj_i = dot / gj_norm2\n",
        "                    proj_j = dot / gi_norm2\n",
        "                    outs[i] = gi - proj_i * gj\n",
        "                    outs[j] = gj - proj_j * gi\n",
        "        return outs\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Evaluation & metrics\n",
        "# ---------------------------\n",
        "@torch.no_grad()\n",
        "def evaluate(model: nn.Module, loader: DataLoader, device: torch.device) -> Tuple[float, float]:\n",
        "    model.eval()\n",
        "    total, correct, loss_sum = 0, 0, 0.0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        logits = model(xb)\n",
        "        loss = F.cross_entropy(logits, yb, reduction='sum')\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct += (preds == yb).sum().item()\n",
        "        total += yb.size(0)\n",
        "        loss_sum += loss.item()\n",
        "    return correct / total, loss_sum / total\n",
        "\n",
        "\n",
        "def compute_drift(global_model: nn.Module, client_param_lists: List[List[torch.Tensor]], device: torch.device) -> float:\n",
        "    with torch.no_grad():\n",
        "        gparams = [p.detach().to(device) for p in global_model.parameters()]\n",
        "        dists = []\n",
        "        for plist in client_param_lists:\n",
        "            s = 0.0\n",
        "            for gp, cp in zip(gparams, plist):\n",
        "                s += torch.norm(cp.to(device) - gp, p=2).item() ** 2\n",
        "            dists.append(math.sqrt(s))\n",
        "        return float(sum(dists) / len(dists))\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Data loading\n",
        "# ---------------------------\n",
        "\n",
        "def get_cifar10(root: str = \"./data\"):\n",
        "    tfm_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "    ])\n",
        "    tfm_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "    ])\n",
        "    train = datasets.CIFAR10(root, train=True, download=True, transform=tfm_train)\n",
        "    test = datasets.CIFAR10(root, train=False, download=True, transform=tfm_test)\n",
        "    return train, test\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fMlOPsnMUrKr"
      },
      "id": "fMlOPsnMUrKr",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #---------------------------\n",
        "# Training orchestration\n",
        "# ---------------------------\n",
        "\n",
        "def run(\n",
        "    strategy: str,\n",
        "    num_clients: int,\n",
        "    alpha: float,\n",
        "    rounds: int,\n",
        "    K: int,\n",
        "    batch_size: int,\n",
        "    lr: float,\n",
        "    momentum: float,\n",
        "    mu: float,\n",
        "    rho: float,\n",
        "    sample_frac: float,\n",
        "    seed: int = 42,\n",
        "    device_str: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "):\n",
        "    set_seed(seed)\n",
        "    device = torch.device(device_str)\n",
        "\n",
        "    # data\n",
        "    train_set, test_set = get_cifar10()\n",
        "    test_loader = DataLoader(test_set, batch_size=256, shuffle=False, num_workers=2)\n",
        "\n",
        "    # partition\n",
        "    targets = torch.tensor(train_set.targets)\n",
        "    splits = dirichlet_partition_indices(targets, num_clients=num_clients, alpha=alpha, seed=seed)\n",
        "\n",
        "    # model template\n",
        "    global_model = SmallCNN().to(device)\n",
        "    server = Server(global_model, device)\n",
        "\n",
        "    # client configs\n",
        "    cfg = ClientConfig(lr=lr, momentum=momentum, batch_size=batch_size, local_epochs=K, mu=mu, rho=rho)\n",
        "\n",
        "    # SCAFFOLD templates\n",
        "    scaffold_template = None\n",
        "    if strategy.lower() == \"scaffold\":\n",
        "        scaffold_template = [p.detach().clone() for p in global_model.parameters()]\n",
        "\n",
        "    # build clients\n",
        "    clients: List[Client] = []\n",
        "    for cid in range(num_clients):\n",
        "        clients.append(\n",
        "            Client(\n",
        "                cid=cid,\n",
        "                dataset=train_set,\n",
        "                indices=splits[cid],\n",
        "                device=device,\n",
        "                cfg=cfg,\n",
        "                strategy=strategy,\n",
        "                model_template=global_model,\n",
        "                scaffold_ci_template=scaffold_template,\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # SCAFFOLD global control variate c\n",
        "    c_global: Optional[List[torch.Tensor]] = None\n",
        "    if strategy.lower() == \"scaffold\":\n",
        "        c_global = [torch.zeros_like(p, device=device) for p in global_model.parameters()]\n",
        "\n",
        "    # training loop\n",
        "    frac = sample_frac\n",
        "    for rnd in range(1, rounds + 1):\n",
        "        # sample participating clients\n",
        "        m = max(1, int(round(frac * num_clients)))\n",
        "        selected = random.sample(range(num_clients), m)\n",
        "\n",
        "        # broadcast implicit via copying in local_train\n",
        "        results = []\n",
        "        for idx in selected:\n",
        "            res = clients[idx].local_train(global_model, c_global)\n",
        "            results.append((idx, res))\n",
        "\n",
        "        # weights by client data size\n",
        "        sizes = [int(res[\"num_samples\"]) for _, res in results]\n",
        "        total = sum(sizes)\n",
        "        weights = [s / total for s in sizes]\n",
        "\n",
        "        # metrics: drift before aggregation (based on current local params)\n",
        "        drift_val = compute_drift(global_model, [res[\"params\"] for _, res in results], device)\n",
        "\n",
        "        # aggregation\n",
        "        if strategy.lower() == \"fedgh\":\n",
        "            # harmonize flat deltas then add to global\n",
        "            flat = []\n",
        "            for _, res in results:\n",
        "                # concat layers (weighted delta will be applied after harmonization via weights)\n",
        "                deltas = res[\"delta\"]\n",
        "                flat.append(torch.cat([d.detach().view(-1).to(device) for d in deltas]))\n",
        "            flat_h = server.harmonize_pairwise(flat)\n",
        "            # reconstruct per-layer from flat\n",
        "            # We'll distribute harmonized flat deltas proportionally by weights\n",
        "            # First, split shapes\n",
        "            shapes = [p.shape for p in global_model.parameters()]\n",
        "            sizes_layer = [int(torch.tensor(s).prod()) for s in shapes]\n",
        "            per_client_deltas: List[List[torch.Tensor]] = []\n",
        "            for fh in flat_h:\n",
        "                offset = 0\n",
        "                dl = []\n",
        "                for sz, shp in zip(sizes_layer, shapes):\n",
        "                    dl.append(fh[offset:offset+sz].view(shp))\n",
        "                    offset += sz\n",
        "                per_client_deltas.append(dl)\n",
        "            server.aggregate_from_deltas(per_client_deltas, weights)\n",
        "        else:\n",
        "            # standard weighted average on parameters (FedAvg-style)\n",
        "            server.aggregate_weighted([res[\"params\"] for _, res in results], weights)\n",
        "\n",
        "        # SCAFFOLD: update c_global to average of ci\n",
        "        if strategy.lower() == \"scaffold\":\n",
        "            with torch.no_grad():\n",
        "                agg_ci = None\n",
        "                for _, res in results:\n",
        "                    ci_list = res[\"ci\"]  # type: ignore\n",
        "                    agg_ci = [t.clone() for t in ci_list] if agg_ci is None else [a + b for a, b in zip(agg_ci, ci_list)]\n",
        "                for i in range(len(agg_ci)):\n",
        "                    agg_ci[i] = agg_ci[i] / len(results)\n",
        "                for i, p in enumerate(c_global):\n",
        "                    p.copy_(agg_ci[i].to(device))\n",
        "\n",
        "        # eval\n",
        "        acc, loss = evaluate(global_model, test_loader, device)\n",
        "        print(f\"Round {rnd:03d} | clients {m:02d}/{num_clients} | drift {drift_val:.3f} | acc {acc*100:.2f}% | loss {loss:.4f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cgY5T2WXUyAt"
      },
      "id": "cgY5T2WXUyAt",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # from ATML_PA4_Task4 import run\n",
        "\n",
        "# strategies = [\n",
        "#     {\"strategy\": \"fedavg\", \"label\": \"FedAvg\"},\n",
        "#     {\"strategy\": \"fedprox\", \"label\": \"FedProx\", \"mu\": 0.01},\n",
        "#     {\"strategy\": \"scaffold\", \"label\": \"SCAFFOLD\"},\n",
        "#     {\"strategy\": \"fedgh\", \"label\": \"FedGH\"},\n",
        "#     {\"strategy\": \"fedsam\", \"label\": \"FedSAM\", \"rho\": 0.05},\n",
        "# ]\n",
        "\n",
        "# common_cfg = dict(\n",
        "#     num_clients=10,\n",
        "#     alpha=0.1,\n",
        "#     rounds=50,\n",
        "#     K=5,\n",
        "#     batch_size=64,\n",
        "#     lr=0.01,\n",
        "#     momentum=0.9,\n",
        "#     sample_frac=1.0,\n",
        "#     seed=42,\n",
        "#     mu=0.0,    # default for non-FedProx\n",
        "#     rho=0.0,   # default for non-FedSAM\n",
        "# )\n",
        "\n",
        "# for cfg in strategies:\n",
        "#     label = cfg.pop(\"label\")\n",
        "#     print(f\"\\n{'='*80}\\nRunning {label}\\n{'='*80}\")\n",
        "#     run(**common_cfg, **cfg)\n"
      ],
      "metadata": {
        "id": "1ZLVywsw2VtZ"
      },
      "id": "1ZLVywsw2VtZ",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Setup: helpers, logging, and parser (run this once) ====\n",
        "import io, sys, os, re, time, json, shutil\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assumes a function run(**kwargs) is already defined in another cell.\n",
        "\n",
        "OUTDIR   = \"pa4_task4_runs\"   # root for artifacts\n",
        "ROUNDS   = 30                 # default rounds (consistent across methods)\n",
        "DOWNLOAD = True               # auto-download zips/plots (Colab)\n",
        "os.makedirs(OUTDIR, exist_ok=True)\n",
        "\n",
        "# Parse lines like: Round 001 | clients 10/10 | drift 3.103 | acc 33.10% | loss 2.1145\n",
        "line_re = re.compile(\n",
        "    r\"Round\\s+(\\d+)\\s+\\|\\s+clients\\s+(\\d+)\\/(\\d+)\\s+\\|\\s+drift\\s+([0-9.]+)\\s+\\|\\s+acc\\s+([0-9.]+)%\\s+\\|\\s+loss\\s+([0-9.]+)\"\n",
        ")\n",
        "\n",
        "def to_jsonable(x):\n",
        "    if isinstance(x, dict): return {str(k): to_jsonable(v) for k,v in x.items()}\n",
        "    if isinstance(x, (list, tuple)): return [to_jsonable(v) for v in x]\n",
        "    if isinstance(x, (str, int, float, bool)) or x is None: return x\n",
        "    try:\n",
        "        import numpy as np\n",
        "        if isinstance(x, np.generic): return x.item()\n",
        "    except: pass\n",
        "    try:\n",
        "        import torch\n",
        "        if isinstance(x, (torch.device, torch.dtype)): return str(x)\n",
        "    except: pass\n",
        "    return str(x)\n",
        "\n",
        "class Tee(io.TextIOBase):\n",
        "    \"\"\"Write to notebook + file simultaneously (live).\"\"\"\n",
        "    def __init__(self, file_obj, mirror): self.f, self.m = file_obj, mirror\n",
        "    def write(self, s): self.m.write(s); self.m.flush(); self.f.write(s); self.f.flush(); return len(s)\n",
        "    def flush(self): self.m.flush(); self.f.flush()\n",
        "\n",
        "def run_and_log(label: str, cfg: dict):\n",
        "    \"\"\"Runs one strategy once, prints live, saves logs/CSV/plots, and optionally zips+downloads.\"\"\"\n",
        "    stamp  = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "    tag    = f\"{label}_alpha{cfg['alpha']}_K{cfg['K']}_N{cfg['num_clients']}_{stamp}\"\n",
        "    run_dir = os.path.join(OUTDIR, tag); os.makedirs(run_dir, exist_ok=True)\n",
        "\n",
        "    # Save config\n",
        "    cfg_to_save = dict(cfg); cfg_to_save[\"label\"] = label\n",
        "    with open(os.path.join(run_dir, \"config.json\"), \"w\") as f: json.dump(to_jsonable(cfg_to_save), f, indent=2)\n",
        "\n",
        "    # Live + file logging\n",
        "    log_path = os.path.join(run_dir, \"train.log\")\n",
        "    with open(log_path, \"w\") as logf:\n",
        "        old = sys.stdout; sys.stdout = Tee(logf, old)\n",
        "        try:\n",
        "            print(\"\\n\" + \"=\"*80); print(f\"Running {label}\"); print(\"=\"*80)\n",
        "            t0 = time.time()\n",
        "            run(**cfg)  # prints will appear live AND be written to train.log\n",
        "            print(f\"[{label}] elapsed: {time.time()-t0:.2f}s\")\n",
        "        finally:\n",
        "            sys.stdout = old\n",
        "\n",
        "    # Parse metrics from train.log\n",
        "    rows = []\n",
        "    with open(log_path) as f:\n",
        "        for line in f:\n",
        "            m = line_re.search(line)\n",
        "            if m:\n",
        "                rows.append((\n",
        "                    int(m.group(1)), int(m.group(2)), int(m.group(3)),\n",
        "                    float(m.group(4)), float(m.group(5))/100.0, float(m.group(6))\n",
        "                ))\n",
        "    df = pd.DataFrame(rows, columns=[\"round\",\"m_clients\",\"n_clients\",\"drift\",\"acc\",\"loss\"])\n",
        "    df.to_csv(os.path.join(run_dir, \"metrics.csv\"), index=False)\n",
        "\n",
        "    # Plots\n",
        "    if not df.empty:\n",
        "        for y, title, name in [\n",
        "            (\"acc\",  f\"{label} — Accuracy vs Rounds\", \"acc_vs_rounds.png\"),\n",
        "            (\"loss\", f\"{label} — Loss vs Rounds\",     \"loss_vs_rounds.png\"),\n",
        "            (\"drift\",f\"{label} — Drift vs Rounds\",    \"drift_vs_rounds.png\"),\n",
        "        ]:\n",
        "            plt.figure(); plt.plot(df[\"round\"], df[y]); plt.xlabel(\"Round\"); plt.ylabel(y.capitalize())\n",
        "            plt.title(title); plt.tight_layout()\n",
        "            plt.savefig(os.path.join(run_dir, name), dpi=150); plt.close()\n",
        "\n",
        "        best = df.loc[df[\"acc\"].idxmax()]\n",
        "        summary = dict(\n",
        "            final_round=int(df[\"round\"].iloc[-1]),\n",
        "            final_acc=float(df[\"acc\"].iloc[-1]),\n",
        "            best_acc=float(best[\"acc\"]),\n",
        "            best_round=int(best[\"round\"]),\n",
        "            final_loss=float(df[\"loss\"].iloc[-1]),\n",
        "            final_drift=float(df[\"drift\"].iloc[-1]),\n",
        "        )\n",
        "        with open(os.path.join(run_dir, \"summary.json\"), \"w\") as f: json.dump(summary, f, indent=2)\n",
        "        print(f\"[{label}] Final acc {summary['final_acc']:.3f} | Best {summary['best_acc']:.3f} @ r{summary['best_round']} | Drift {summary['final_drift']:.3f}\")\n",
        "    else:\n",
        "        print(f\"[{label}] WARN: no metrics parsed. Check train.log format.\")\n",
        "\n",
        "    # Zip + optional download\n",
        "    zip_path = shutil.make_archive(run_dir, \"zip\", run_dir)\n",
        "    if DOWNLOAD:\n",
        "        try:\n",
        "            from google.colab import files\n",
        "            files.download(zip_path)\n",
        "        except Exception as e:\n",
        "            print(f\"[{label}] Download skipped ({e}). Zip at: {zip_path}\")\n",
        "\n",
        "    return run_dir\n"
      ],
      "metadata": {
        "id": "PbE3HLKzkctn"
      },
      "id": "PbE3HLKzkctn",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg_fedavg = dict(\n",
        "    strategy=\"fedavg\", num_clients=10, alpha=0.1, rounds=ROUNDS, K=5,\n",
        "    batch_size=64, lr=0.01, momentum=0.9, sample_frac=1.0, seed=42, mu=0.0, rho=0.0\n",
        ")\n",
        "run_dir_fedavg = run_and_log(\"FedAvg\", cfg_fedavg)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HT-t5lKZA5T6",
        "outputId": "23fea865-f879-43fc-81be-8a98bf0e53ed"
      },
      "id": "HT-t5lKZA5T6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "Running FedAvg\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:04<00:00, 42.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cfg_fedprox = dict(\n",
        "    strategy=\"fedprox\", num_clients=10, alpha=0.1, rounds=ROUNDS, K=5,\n",
        "    batch_size=64, lr=0.01, momentum=0.9, sample_frac=1.0, seed=42, mu=0.01, rho=0.0\n",
        ")\n",
        "run_dir_fedprox = run_and_log(\"FedProx\", cfg_fedprox)\n"
      ],
      "metadata": {
        "id": "etP2jqG5Q3nx"
      },
      "id": "etP2jqG5Q3nx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# combined accuracy plot\n",
        "if all_curves:\n",
        "    comb = pd.concat(all_curves, ignore_index=True)\n",
        "    plt.figure()\n",
        "    for label, grp in comb.groupby(\"label\"):\n",
        "        plt.plot(grp[\"round\"], grp[\"acc\"], label=label)\n",
        "    plt.xlabel(\"Round\"); plt.ylabel(\"Accuracy\")\n",
        "    plt.title(f\"Accuracy vs Rounds — All Strategies ({common_cfg['rounds']} rounds)\")\n",
        "    plt.legend(); plt.tight_layout()\n",
        "    combo_path = os.path.join(OUTDIR, f\"combined_accuracy_{common_cfg['rounds']}r.png\")\n",
        "    plt.savefig(combo_path, dpi=160); plt.close()\n",
        "    print(f\"Combined accuracy plot saved -> {combo_path}\")\n",
        "\n",
        "print(f\"All artifacts under: {os.path.abspath(OUTDIR)}\")"
      ],
      "metadata": {
        "id": "MXtiPYhkA4UJ"
      },
      "id": "MXtiPYhkA4UJ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}